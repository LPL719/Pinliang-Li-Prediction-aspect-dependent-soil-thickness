{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0edef2-33c9-4727-bba7-19fc101d3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Uncertainty analysis for XGB(ADF)\n",
    "---------------------------------\n",
    "- Reads all HHO-optimized configurations (typically 100 rows) from HHO_XGB_results_SSSI.xlsx\n",
    "- For each config: split data 70/30 with the same iteration-based random_state, fit XGB,\n",
    "  and collect predictions paired with their *true* thickness values (train + test).\n",
    "- For each unique thickness value, compute mean, 2.5th and 97.5th percentiles -> 95% CI.\n",
    "- Exports a small figure of the mean curve with shaded 95% CI.\n",
    "\n",
    "This matches the manuscript’s uncertainty idea and keeps the core training choices consistent.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- paths ---\n",
    "PARAM_XLSX = \"HHO_XGB_results_SSSI.xlsx\"   # results with 100 rows for SSSI group\n",
    "TRAIN_XLSX = \"Train_data.xlsx\"             # training data with 7 features + 'thickness'\n",
    "OUT_FIG    = \"uncertainty_ci.png\"\n",
    "\n",
    "# --- features/target ---\n",
    "FEATURES = ['SSSI', 'SSP', 'DEM', 'MRRTF', 'NDVI', 'MRVBF', 'RD']\n",
    "TARGET   = 'thickness'\n",
    "\n",
    "# --- load data ---\n",
    "df_params = pd.read_excel(PARAM_XLSX)\n",
    "df_data   = pd.read_excel(TRAIN_XLSX).dropna()\n",
    "\n",
    "# filter to SSSI group if column exists\n",
    "if \"Group\" in df_params.columns:\n",
    "    df_params = df_params[df_params[\"Group\"] == \"SSSI\"].copy()\n",
    "\n",
    "# sanity checks\n",
    "need_cols = set(FEATURES + [TARGET])\n",
    "missing = [c for c in need_cols if c not in df_data.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in training data: {missing}\")\n",
    "if df_params.empty:\n",
    "    raise ValueError(\"No parameter rows found for SSSI in the results file.\")\n",
    "\n",
    "X_all = df_data[FEATURES].values\n",
    "y_all = df_data[TARGET].values\n",
    "\n",
    "# map: true thickness -> list of predicted values across all runs\n",
    "unique_thk = np.unique(y_all).astype(float)\n",
    "pred_bag = {float(t): [] for t in unique_thk}\n",
    "\n",
    "# iterate all HHO configurations (typically 100)\n",
    "for idx, row in tqdm(df_params.iterrows(), total=len(df_params), desc=\"Uncertainty runs\"):\n",
    "    # seed: use iteration directly if present; else fall back to row index + 1\n",
    "    if \"Iteration\" in row and not pd.isna(row[\"Iteration\"]):\n",
    "        seed0 = int(row[\"Iteration\"])\n",
    "    else:\n",
    "        seed0 = int(idx) + 1\n",
    "\n",
    "    model = XGBRegressor(\n",
    "        max_depth=int(row['max_depth']),\n",
    "        learning_rate=float(row['learning_rate']),\n",
    "        n_estimators=int(row['n_estimators']),\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=seed0,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # use 70/30 split to match the manuscript’s repeated training setup\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_all, train_size=0.7, random_state=seed0)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # collect predictions with their ground-truth thickness\n",
    "    yhat_tr = model.predict(X_tr)\n",
    "    yhat_te = model.predict(X_te)\n",
    "    for t_true, yhat in zip(y_tr, yhat_tr):\n",
    "        pred_bag[float(t_true)].append(float(yhat))\n",
    "    for t_true, yhat in zip(y_te, yhat_te):\n",
    "        pred_bag[float(t_true)].append(float(yhat))\n",
    "\n",
    "# aggregate to mean and 95% CI per thickness\n",
    "thk_list, mean_list, lo_list, hi_list = [], [], [], []\n",
    "for t in sorted(pred_bag.keys()):\n",
    "    preds = np.array(pred_bag[t], dtype=float)\n",
    "    if preds.size == 0:\n",
    "        continue\n",
    "    thk_list.append(t)\n",
    "    mean_list.append(preds.mean())\n",
    "    lo_list.append(np.percentile(preds, 2.5))\n",
    "    hi_list.append(np.percentile(preds, 97.5))\n",
    "\n",
    "thicknesses = np.array(thk_list)\n",
    "means       = np.array(mean_list)\n",
    "los         = np.array(lo_list)\n",
    "his         = np.array(hi_list)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(3.67, 2.52))\n",
    "plt.fill_between(thicknesses, los, his, alpha=0.3, label=\"95% Confidence Interval\")\n",
    "plt.plot(thicknesses, means, linewidth=1, label=\"Mean Prediction\")\n",
    "\n",
    "plt.xlabel(\"Soil Thickness (m)\", fontsize=9)\n",
    "plt.ylabel(\"Prediction (m)\", fontsize=9)\n",
    "plt.grid(True, linewidth=0.4, alpha=0.4)\n",
    "plt.legend(frameon=False, loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_FIG, dpi=600, bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "\n",
    "print(f\"[DONE] Uncertainty curve saved -> {OUT_FIG}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
